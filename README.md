# Insurance Reasoning Engine

A powerful, flexible AI-powered document analysis system that can understand and answer questions about any uploaded document using natural language processing, semantic search, and large language models.

## ğŸš€ Features

- **Universal Document Support**: Upload and analyze PDFs, DOCX files, and EML emails
- **Natural Language Queries**: Ask any question in plain English about your documents
- **Intelligent Retrieval**: Advanced semantic search using FAISS and HuggingFace embeddings
- **Comprehensive Responses**: Detailed answers with source references and confidence levels
- **Flexible Architecture**: Works with any type of document, not just insurance policies
- **RESTful API**: Easy-to-use FastAPI endpoints with automatic documentation
- **Production Ready**: Includes logging, error handling, and deployment configurations

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip (Python package installer)
- An OpenAI API key or OpenRouter API key

## ğŸ› ï¸ Installation

### 1. Clone the Repository
```bash
git clone <repository-url>
cd insurance-ai-engine
```

### 2. Create a Virtual Environment (Recommended)
```bash
# On Windows
python -m venv .venv
.venv\Scripts\activate

# On macOS/Linux
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Set Up Environment Variables
Create a `.env` file in the project root:
```env
# Choose your LLM provider
LLM_PROVIDER=openrouter  # or openai

# API Keys (get one from the provider you chose)
OPENAI_API_KEY=your_openai_key_here
OPENROUTER_API_KEY=your_openrouter_key_here

# Optional: Customize embedding model
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
```

**Getting API Keys:**
- **OpenAI**: Visit [OpenAI Platform](https://platform.openai.com/) to get an API key
- **OpenRouter**: Visit [OpenRouter](https://openrouter.ai/) for free and paid models

## ğŸš€ Quick Start

### 1. Start the Server
```bash
uvicorn main:app --reload
```

The API will be available at:
- **API**: http://127.0.0.1:8000
- **Interactive Docs**: http://127.0.0.1:8000/docs

### 2. Upload a Document
Use the `/upload-docs` endpoint to upload your first document:
- Go to http://127.0.0.1:8000/docs
- Click on `POST /upload-docs`
- Click "Try it out"
- Upload a PDF, DOCX, or EML file
- You'll receive a `doc_id` upon successful upload

### 3. Ask Questions
Use the `/ask-query` endpoint to ask questions about your document:
```json
{
  "query": "What is the grace period for premium payment?"
}
```

## ğŸ“š API Endpoints

### `POST /upload-docs`
Upload and index a document for analysis.

**Request:**
- Content-Type: `multipart/form-data`
- Body: File upload (PDF, DOCX, EML)

**Response:**
```json
{
  "doc_id": "f4c532bd-88b4-4de1-bdf1-ea64b50771ad"
}
```

### `POST /ask-query`
Ask any question about uploaded documents.

**Request:**
```json
{
  "query": "What is the waiting period for pre-existing diseases?"
}
```

**Response:**
```json
{
  "answer": "Comprehensive answer with full context and explanations...",
  "confidence": "high",
  "source_sections": [
    {
      "section": "Section 3.2",
      "content": "Relevant text from the document...",
      "relevance": "Detailed explanation of why this section is relevant..."
    }
  ],
  "additional_info": "Additional context, practical implications, and helpful guidance..."
}
```

### `GET /clauses/{id}`
Retrieve the full text of a specific document chunk.

### `GET /debug/chunks`
List all available document chunks (for debugging).

### `GET /health`
Health check endpoint.

## ğŸ’¡ Usage Examples

### Insurance Policy Analysis
```json
{
  "query": "What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?"
}
```

### General Document Questions
```json
{
  "query": "What are the main benefits of this policy?"
}
```

### Specific Coverage Questions
```json
{
  "query": "Does this policy cover maternity expenses, and what are the conditions?"
}
```

### Claim Process Questions
```json
{
  "query": "How do I file a claim and what documents do I need?"
}
```

## ğŸ—ï¸ Project Structure

```
insurance-ai-engine/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ asgi.py                 # ASGI entry point for deployment
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env                    # Environment variables (create this)
â”œâ”€â”€ vercel.json            # Vercel deployment configuration
â”œâ”€â”€ README.md              # This file
â”‚
â”œâ”€â”€ routes/                 # API route handlers
â”‚   â”œâ”€â”€ upload.py          # Document upload endpoint
â”‚   â”œâ”€â”€ query.py           # Query processing endpoint
â”‚   â””â”€â”€ clauses.py         # Clause retrieval endpoint
â”‚
â”œâ”€â”€ services/              # Core business logic
â”‚   â”œâ”€â”€ doc_ingestion_service.py    # Document processing
â”‚   â”œâ”€â”€ embedding_service.py        # Embedding generation and FAISS
â”‚   â””â”€â”€ query_reasoning_service.py  # Query analysis and LLM integration
â”‚
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”œâ”€â”€ parser_utils.py    # Document parsing (PDF, DOCX, EML)
â”‚   â”œâ”€â”€ text_splitter.py   # Semantic text chunking
â”‚   â”œâ”€â”€ prompt_templates.py # LLM prompt templates
â”‚   â””â”€â”€ logging_utils.py   # Logging configuration
â”‚
â”œâ”€â”€ data/                  # Data storage
â”‚   â”œâ”€â”€ uploaded_docs/     # Original uploaded files
â”‚   â””â”€â”€ faiss_index/       # FAISS vector index and metadata
â”‚
â””â”€â”€ tests/                 # Test files
    â”œâ”€â”€ test_upload.py
    â”œâ”€â”€ test_query.py
    â””â”€â”€ test_embeddings.py
```

## ğŸ”§ Configuration

### Environment Variables
| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_PROVIDER` | Choose between 'openai' or 'openrouter' | 'openrouter' |
| `OPENAI_API_KEY` | Your OpenAI API key | Required if using OpenAI |
| `OPENROUTER_API_KEY` | Your OpenRouter API key | Required if using OpenRouter |
| `EMBEDDING_MODEL` | HuggingFace embedding model | 'sentence-transformers/all-MiniLM-L6-v2' |

### Customization Options
- **Chunk Size**: Modify `chunk_size` in `utils/text_splitter.py`
- **Retrieval Count**: Adjust `top_k` in `services/query_reasoning_service.py`
- **LLM Model**: Change the model in the `_call_llm` method

## ğŸ§ª Testing

Run the test suite:
```bash
pytest tests/
```

## ğŸš€ Deployment

### Local Development
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Vercel Deployment
1. Install Vercel CLI: `npm i -g vercel`
2. Deploy: `vercel --prod`

### Docker Deployment
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## ğŸ” Troubleshooting

### Common Issues

**1. "No relevant information found"**
- Check if the document was uploaded successfully
- Try rephrasing your question with different keywords
- Use `/debug/chunks` to see available content
- Ensure the document contains the information you're asking about

**2. API Key Errors**
- Verify your API key is correct in `.env`
- Check if you have sufficient credits/quota
- Ensure the LLM provider is set correctly

**3. Document Upload Fails**
- Check file format (PDF, DOCX, EML only)
- Ensure file is not corrupted
- Check server logs for specific error messages

**4. Poor Response Quality**
- Try asking more specific questions
- Check if the document contains relevant information
- Consider using a better LLM model

### Debug Endpoints
- `/debug/chunks`: List all available document chunks
- `/health`: Check system status
- Server logs: Check terminal output for detailed information

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

If you encounter issues:
1. Check the troubleshooting section above
2. Review server logs for error messages
3. Test with the debug endpoints
4. Open an issue with detailed information about the problem

## ğŸ¯ Use Cases

This system is perfect for:
- **Insurance Companies**: Analyze policies and answer customer questions
- **Legal Firms**: Review contracts and legal documents
- **Healthcare**: Process medical policies and procedures
- **Education**: Analyze textbooks and educational materials
- **Business**: Review company policies and procedures
- **Research**: Analyze research papers and reports

The system is designed to be flexible and can work with any type of document where you need to extract and understand information through natural language queries. 